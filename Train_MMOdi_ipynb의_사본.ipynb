{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ByeonJaeseong/DeepLearningProject/blob/main/Train_MMOdi_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WAYnybdRZp7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5313d7b-f895-445f-bff7-282cb1ef73e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m122.9/176.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n",
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.2.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5aea59bcf2a2>:12: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-74752c50-6918-4352-98b9-a20f91da0d77\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-74752c50-6918-4352-98b9-a20f91da0d77\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving answer_sample.csv to answer_sample.csv\n",
            "Saving data_c30.csv to data_c30.csv\n",
            "Saving data_c40.csv to data_c40.csv\n",
            "Saving data_c50.csv to data_c50.csv\n",
            "Saving data_c70.csv to data_c70.csv\n",
            "Saving data_c100.csv to data_c100.csv\n",
            "Saving data_columns.csv to data_columns.csv\n",
            "Saving data_s30.csv to data_s30.csv\n",
            "Saving data_s40.csv to data_s40.csv\n",
            "Saving data_s50.csv to data_s50.csv\n",
            "Saving data_s70.csv to data_s70.csv\n",
            "Saving data_s100.csv to data_s100.csv\n",
            "Saving lane_data_c.csv to lane_data_c.csv\n",
            "Saving lane_data_columns.csv to lane_data_columns.csv\n",
            "Saving lane_data_s.csv to lane_data_s.csv\n",
            "Saving 차량_및_요댐퍼.xlsx to 차량_및_요댐퍼.xlsx\n"
          ]
        }
      ],
      "source": [
        "!pip install -U keras-tuner\n",
        "!pip install bayesian-optimization\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from bayes_opt import BayesianOptimization\n",
        "uploaded = files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m2XbpiyoVDp7"
      },
      "outputs": [],
      "source": [
        "# # 사용자 정의 Weighted MAPE 손실 함수\n",
        "# def weighted_mape(y_true, y_pred):\n",
        "#     # Calculate the absolute errors and absolute true values\n",
        "#     absolute_errors = np.abs(y_true - y_pred)\n",
        "#     absolute_true = np.abs(y_true)\n",
        "\n",
        "#     # Calculate the weights based on the time steps\n",
        "#     weights = np.linspace(1.0001, 1.1999, len(y_true))\n",
        "\n",
        "#     # Calculate the weighted MAPE components\n",
        "#     weighted_mape_components = weights * absolute_errors / absolute_true / len(y_true)\n",
        "\n",
        "#     # Sum up the weighted components and multiply by 100\n",
        "#     weighted_mape = np.sum(weighted_mape_components) * 100\n",
        "\n",
        "#     return weighted_mape\n",
        "\n",
        "# 사용자 정의 Weighted MAPE 손실 함수\n",
        "def weighted_mape_loss(y_true, y_pred):\n",
        "    # Calculate the absolute errors and absolute true values\n",
        "    absolute_errors = tf.abs(y_true - y_pred)\n",
        "    absolute_true = tf.abs(y_true)\n",
        "\n",
        "    # Calculate the weights based on the time steps\n",
        "    weights = tf.linspace(1.0001, 1.1999, tf.shape(y_true)[0])\n",
        "    weights = tf.expand_dims(weights, axis=-1)  # Expand dimensions to match y_true\n",
        "\n",
        "     #Calculate the weighted MAPE components\n",
        "    epsilon = 1e-5  # Small constant to avoid division by zero\n",
        "    weighted_mape_components = (weights * absolute_errors) / (absolute_true + epsilon)\n",
        "\n",
        "    weighted_mape = tf.reduce_sum(weighted_mape_components)*100 / tf.cast(tf.shape(y_true)[0], tf.float32)\n",
        "    return weighted_mape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mHnEyJj-xc4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWFrJHP_97SE"
      },
      "outputs": [],
      "source": [
        "# 사용자 정의 Weighted MAPE 손실 함수\n",
        "def weighted_mape_loss(y_true, y_pred):\n",
        "    # Calculate the absolute errors and absolute true values\n",
        "    absolute_errors = tf.abs(y_true - y_pred)\n",
        "    absolute_true = tf.abs(y_true)\n",
        "\n",
        "    # Calculate the weights based on the time steps\n",
        "    weights = tf.linspace(1.0001, 1.1999, tf.shape(y_true)[0])\n",
        "    weights = tf.expand_dims(weights, axis=-1)  # Expand dimensions to match y_true\n",
        "\n",
        "    # Calculate the weighted MAPE components\n",
        "    weighted_mape_components = weights * absolute_errors / absolute_true\n",
        "\n",
        "    # Sum up the weighted components and multiply by 100\n",
        "    weighted_mape = tf.reduce_sum(weighted_mape_components)/ tf.cast(tf.shape(y_true)[0], tf.float32) * 100\n",
        "    return weighted_mape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4wvurYuKWKR7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "703eb5a3-c167-465a-d290-459c73caf58f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-42ced2e43bc0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#튜너 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tuner_results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"
          ]
        }
      ],
      "source": [
        "#튜너 삭제\n",
        "shutil.rmtree('tuner_results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWhLbFbpZ6X6",
        "outputId": "dcfc4c17-904c-4b5d-e7fd-288cb6943c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 5\n",
            "lstm_units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 128, 'step': 8, 'sampling': 'linear'}\n",
            "dropout_rate (Float)\n",
            "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.005, 'sampling': 'linear'}\n",
            "num_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
            "optimizer (Choice)\n",
            "{'default': 'adam', 'conditions': [], 'values': ['adam', 'rmsprop'], 'ordered': False}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.1, 'conditions': [], 'values': [0.1, 0.01, 0.001, 0.0001], 'ordered': True}\n",
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "112               |112               |lstm_units\n",
            "0.01              |0.01              |dropout_rate\n",
            "1                 |1                 |num_layers\n",
            "rmsprop           |rmsprop           |optimizer\n",
            "0.001             |0.001             |learning_rate\n",
            "\n",
            "Epoch 1/1000\n",
            "600/600 [==============================] - 13s 13ms/step - loss: 8225.9570 - val_loss: 1538877.3750\n",
            "Epoch 2/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 4901.2896 - val_loss: 1786696.1250\n",
            "Epoch 3/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 4192.4536 - val_loss: 355581.7812\n",
            "Epoch 4/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 3768.4375 - val_loss: 547571.2500\n",
            "Epoch 5/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 3364.7100 - val_loss: 283726.8750\n",
            "Epoch 6/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 3144.2610 - val_loss: 524574.0625\n",
            "Epoch 7/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 2925.0779 - val_loss: 844162.5625\n",
            "Epoch 8/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2735.1836 - val_loss: 903437.8750\n",
            "Epoch 9/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 2410.7339 - val_loss: 1058578.6250\n",
            "Epoch 10/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2397.8101 - val_loss: 450326.7812\n",
            "Epoch 11/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 2285.0413 - val_loss: 255240.7500\n",
            "Epoch 12/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 2253.2507 - val_loss: 680214.0000\n",
            "Epoch 13/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2154.3992 - val_loss: 286370.0312\n",
            "Epoch 14/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2051.4338 - val_loss: 342224.1250\n",
            "Epoch 15/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 2039.2164 - val_loss: 230227.8438\n",
            "Epoch 16/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1985.7394 - val_loss: 380327.3125\n",
            "Epoch 17/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2008.0592 - val_loss: 353699.6562\n",
            "Epoch 18/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1882.4725 - val_loss: 517836.7500\n",
            "Epoch 19/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1807.9464 - val_loss: 296711.2188\n",
            "Epoch 20/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1845.1949 - val_loss: 160503.8594\n",
            "Epoch 21/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1771.7554 - val_loss: 911975.0625\n",
            "Epoch 22/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1742.1610 - val_loss: 393760.0938\n",
            "Epoch 23/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1721.8839 - val_loss: 525183.2500\n",
            "Epoch 24/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1764.9873 - val_loss: 380565.5000\n",
            "Epoch 25/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1778.1625 - val_loss: 158660.7656\n",
            "Epoch 26/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1649.6636 - val_loss: 436218.5625\n",
            "Epoch 27/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1633.5837 - val_loss: 489894.1875\n",
            "Epoch 28/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1657.8348 - val_loss: 351227.0625\n",
            "Epoch 29/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1601.0332 - val_loss: 603091.8125\n",
            "Epoch 30/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1503.3781 - val_loss: 602271.8125\n",
            "Epoch 31/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1486.0817 - val_loss: 241057.7031\n",
            "Epoch 32/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1497.6638 - val_loss: 525976.0625\n",
            "Epoch 33/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1558.3009 - val_loss: 358168.8750\n",
            "Epoch 34/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1529.9507 - val_loss: 189402.1406\n",
            "Epoch 35/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1454.2880 - val_loss: 422728.8438\n",
            "Epoch 36/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1471.1758 - val_loss: 358595.8125\n",
            "Epoch 37/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1455.5095 - val_loss: 220856.3594\n",
            "Epoch 38/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1471.9073 - val_loss: 356213.9375\n",
            "Epoch 39/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 1445.2921 - val_loss: 210389.7344\n",
            "Epoch 40/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1468.8036 - val_loss: 636857.6875\n",
            "Epoch 41/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1417.1317 - val_loss: 144811.5625\n",
            "Epoch 42/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1388.2052 - val_loss: 252557.6875\n",
            "Epoch 43/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1393.6506 - val_loss: 121990.6172\n",
            "Epoch 44/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1389.0623 - val_loss: 335427.7188\n",
            "Epoch 45/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1376.6279 - val_loss: 576699.6250\n",
            "Epoch 46/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 1338.2000 - val_loss: 382331.1875\n",
            "Epoch 47/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1384.8993 - val_loss: 183863.5000\n",
            "Epoch 48/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1333.2427 - val_loss: 208417.5469\n",
            "Epoch 49/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1374.6438 - val_loss: 133136.0156\n",
            "Epoch 50/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1375.3016 - val_loss: 131364.8906\n",
            "Epoch 51/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 1349.8511 - val_loss: 296696.4688\n",
            "Epoch 52/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1434.0941 - val_loss: 160755.2656\n",
            "Epoch 53/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1385.7821 - val_loss: 183839.8281\n",
            "Epoch 54/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1352.6161 - val_loss: 308897.6562\n",
            "Epoch 55/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1377.5317 - val_loss: 201178.4062\n",
            "Epoch 56/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1370.0863 - val_loss: 590253.2500\n",
            "Epoch 57/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1415.5645 - val_loss: 212004.6875\n",
            "Epoch 58/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1398.0913 - val_loss: 299355.0938\n",
            "Epoch 59/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1362.7130 - val_loss: 148310.9375\n",
            "Epoch 60/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 1353.2277 - val_loss: 488915.2500\n",
            "Epoch 61/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1380.5647 - val_loss: 218481.4062\n",
            "Epoch 62/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 1379.7284 - val_loss: 154007.0625\n",
            "Epoch 63/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1370.5125 - val_loss: 422922.5312\n",
            "Epoch 64/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1418.8257 - val_loss: 208499.1875\n",
            "Epoch 65/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1400.2373 - val_loss: 143089.2812\n",
            "Epoch 66/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1372.4946 - val_loss: 240853.4062\n",
            "Epoch 67/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1356.3292 - val_loss: 432628.9688\n",
            "Epoch 68/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1373.4230 - val_loss: 257358.7969\n",
            "Epoch 69/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1436.5189 - val_loss: 204970.5469\n",
            "Epoch 70/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1321.8741 - val_loss: 171286.2344\n",
            "Epoch 71/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1366.8640 - val_loss: 354969.5312\n",
            "Epoch 72/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1362.7993 - val_loss: 383300.3750\n",
            "Epoch 73/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1348.1680 - val_loss: 470154.7812\n",
            "Epoch 74/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 1393.4919 - val_loss: 133892.5000\n",
            "Epoch 75/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1380.5828 - val_loss: 137338.2031\n",
            "Epoch 76/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1398.0530 - val_loss: 203228.2344\n",
            "Epoch 77/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1361.0754 - val_loss: 600304.6875\n",
            "Epoch 78/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1371.2766 - val_loss: 293081.0625\n",
            "Epoch 79/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1388.2332 - val_loss: 350343.2188\n",
            "Epoch 80/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1371.5576 - val_loss: 285662.6250\n",
            "Epoch 81/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1401.7505 - val_loss: 133364.3438\n",
            "Epoch 82/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1356.5405 - val_loss: 187609.5469\n",
            "Epoch 83/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1386.8558 - val_loss: 608791.8125\n",
            "Epoch 84/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1404.1161 - val_loss: 410444.9062\n",
            "Epoch 85/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1405.3430 - val_loss: 402176.4688\n",
            "Epoch 86/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1320.8777 - val_loss: 157380.9219\n",
            "Epoch 87/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1339.2063 - val_loss: 174118.2656\n",
            "Epoch 88/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1335.6909 - val_loss: 280689.9062\n",
            "Epoch 89/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1336.8162 - val_loss: 212342.3438\n",
            "Epoch 90/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1365.4170 - val_loss: 186423.1094\n",
            "Epoch 91/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1309.4205 - val_loss: 453306.7188\n",
            "Epoch 92/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1393.4747 - val_loss: 318666.0000\n",
            "Epoch 93/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1349.3156 - val_loss: 161932.2812\n",
            "Epoch 94/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1305.7081 - val_loss: 391042.9688\n",
            "Epoch 95/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1285.0620 - val_loss: 235995.4375\n",
            "Epoch 96/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1310.9547 - val_loss: 401511.2812\n",
            "Epoch 97/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1295.2040 - val_loss: 192428.4375\n",
            "Epoch 98/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1311.5686 - val_loss: 435178.2188\n",
            "Epoch 99/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1288.9917 - val_loss: 320396.6250\n",
            "Epoch 100/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1283.3149 - val_loss: 447865.6562\n",
            "Epoch 101/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1251.5125 - val_loss: 745657.9375\n",
            "Epoch 102/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1285.6064 - val_loss: 320888.6562\n",
            "Epoch 103/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1280.5262 - val_loss: 393765.5312\n",
            "Epoch 104/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1277.1205 - val_loss: 434873.6875\n",
            "Epoch 105/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1278.0168 - val_loss: 248402.1094\n",
            "Epoch 106/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1297.1163 - val_loss: 371659.4375\n",
            "Epoch 107/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1252.3704 - val_loss: 429824.0938\n",
            "Epoch 108/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1303.2531 - val_loss: 515881.3750\n",
            "Epoch 109/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1332.4934 - val_loss: 234114.1094\n",
            "Epoch 110/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1274.0303 - val_loss: 285844.5000\n",
            "Epoch 111/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1257.7390 - val_loss: 915685.0000\n",
            "Epoch 112/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1288.3395 - val_loss: 283682.5000\n",
            "Epoch 113/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1263.6256 - val_loss: 235964.4531\n",
            "Epoch 114/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1289.7764 - val_loss: 474541.0000\n",
            "Epoch 115/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1309.4703 - val_loss: 409031.2188\n",
            "Epoch 116/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1290.9722 - val_loss: 217358.4219\n",
            "Epoch 117/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1288.5164 - val_loss: 318977.8438\n",
            "Epoch 118/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1264.3995 - val_loss: 333045.6875\n",
            "Epoch 119/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1228.0238 - val_loss: 241343.1406\n",
            "Epoch 120/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1282.8308 - val_loss: 147371.7812\n",
            "Epoch 121/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1296.6704 - val_loss: 353976.1250\n",
            "Epoch 122/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1302.6813 - val_loss: 352685.3438\n",
            "Epoch 123/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1236.2506 - val_loss: 199029.4531\n",
            "Epoch 124/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1295.3268 - val_loss: 541482.3750\n",
            "Epoch 125/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1312.5997 - val_loss: 349365.4688\n",
            "Epoch 126/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1280.6548 - val_loss: 547146.8750\n",
            "Epoch 127/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1286.5337 - val_loss: 288966.2812\n",
            "Epoch 128/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1273.4362 - val_loss: 258985.6562\n",
            "Epoch 129/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1252.3585 - val_loss: 603096.5000\n",
            "Epoch 130/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1324.5709 - val_loss: 262156.5312\n",
            "Epoch 131/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1269.2838 - val_loss: 321613.0000\n",
            "Epoch 132/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1285.0137 - val_loss: 390554.7500\n",
            "Epoch 133/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1318.2802 - val_loss: 339192.5312\n",
            "Epoch 134/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1268.0916 - val_loss: 280155.1250\n",
            "Epoch 135/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1301.6841 - val_loss: 538843.4375\n",
            "Epoch 136/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1257.6182 - val_loss: 562526.6250\n",
            "Epoch 137/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1291.4194 - val_loss: 406945.9375\n",
            "Epoch 138/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1357.7141 - val_loss: 315047.8750\n",
            "Epoch 139/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1312.0322 - val_loss: 490522.4688\n",
            "Epoch 140/1000\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 1258.9137 - val_loss: 548403.2500\n",
            "Epoch 141/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1303.7557 - val_loss: 222163.7188\n",
            "Epoch 142/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1252.5000 - val_loss: 283215.9062\n",
            "Epoch 143/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1270.6301 - val_loss: 440622.6250\n",
            "Epoch 144/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1291.9847 - val_loss: 396769.7500\n",
            "Epoch 145/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1324.8800 - val_loss: 442170.3125\n",
            "Epoch 146/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1267.0311 - val_loss: 274015.4688\n",
            "Epoch 147/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1268.2426 - val_loss: 268380.0938\n",
            "Epoch 148/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1224.5730 - val_loss: 207280.3438\n",
            "Epoch 149/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1270.4805 - val_loss: 294688.4688\n",
            "Epoch 150/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1236.3297 - val_loss: 504218.7188\n",
            "Epoch 151/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1282.2089 - val_loss: 292472.3125\n",
            "Epoch 152/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1312.1616 - val_loss: 253981.4375\n",
            "Epoch 153/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1308.7446 - val_loss: 402302.3750\n",
            "Epoch 154/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1237.0884 - val_loss: 589596.5625\n",
            "Epoch 155/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1308.7316 - val_loss: 504717.4375\n",
            "Epoch 156/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1306.6324 - val_loss: 233128.6406\n",
            "Epoch 157/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1318.3188 - val_loss: 406849.5000\n",
            "Epoch 158/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1295.8635 - val_loss: 278789.3750\n",
            "Epoch 159/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1251.8015 - val_loss: 254229.0469\n",
            "Epoch 160/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1260.7218 - val_loss: 614913.6250\n",
            "Epoch 161/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1365.8176 - val_loss: 492507.8438\n",
            "Epoch 162/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1247.6221 - val_loss: 333812.8438\n",
            "Epoch 163/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1273.3530 - val_loss: 245137.8594\n",
            "Epoch 164/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1288.4343 - val_loss: 527278.8750\n",
            "Epoch 165/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1260.3932 - val_loss: 779138.5000\n",
            "Epoch 166/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1353.7848 - val_loss: 347543.8125\n",
            "Epoch 167/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1298.6211 - val_loss: 204480.1562\n",
            "Epoch 168/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1320.9247 - val_loss: 344510.0312\n",
            "Epoch 169/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1255.0829 - val_loss: 285928.0938\n",
            "Epoch 170/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1310.9841 - val_loss: 279578.3438\n",
            "Epoch 171/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1246.4054 - val_loss: 427185.7500\n",
            "Epoch 172/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1303.9031 - val_loss: 559059.1250\n",
            "Epoch 173/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1325.4604 - val_loss: 579231.7500\n",
            "Epoch 174/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1302.7333 - val_loss: 367363.1875\n",
            "Epoch 175/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1261.0137 - val_loss: 316460.3125\n",
            "Epoch 176/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1325.5715 - val_loss: 439197.3438\n",
            "Epoch 177/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1309.6732 - val_loss: 366831.9062\n",
            "Epoch 178/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1339.8270 - val_loss: 404805.8125\n",
            "Epoch 179/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1294.9655 - val_loss: 253046.5312\n",
            "Epoch 180/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1307.7203 - val_loss: 333359.9062\n",
            "Epoch 181/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1225.6776 - val_loss: 432716.8438\n",
            "Epoch 182/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1254.9811 - val_loss: 507943.7188\n",
            "Epoch 183/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1281.7863 - val_loss: 288033.8125\n",
            "Epoch 184/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1298.1572 - val_loss: 519000.5938\n",
            "Epoch 185/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1285.8712 - val_loss: 444346.2500\n",
            "Epoch 186/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1287.6686 - val_loss: 461225.1250\n",
            "Epoch 187/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1289.3527 - val_loss: 303987.9375\n",
            "Epoch 188/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1300.1228 - val_loss: 523528.3125\n",
            "Epoch 189/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1281.4233 - val_loss: 309470.5938\n",
            "Epoch 190/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1279.5315 - val_loss: 282383.0000\n",
            "Epoch 191/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1288.9485 - val_loss: 389415.0625\n",
            "Epoch 192/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1255.8632 - val_loss: 321221.0000\n",
            "Epoch 193/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1266.6353 - val_loss: 499363.2500\n",
            "Epoch 194/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1288.0957 - val_loss: 389532.1562\n",
            "Epoch 195/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1270.3922 - val_loss: 329470.3438\n",
            "Epoch 196/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1295.1146 - val_loss: 284475.1875\n",
            "Epoch 197/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1301.8677 - val_loss: 337224.8125\n",
            "Epoch 198/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1381.3586 - val_loss: 472693.2812\n",
            "Epoch 199/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1258.9108 - val_loss: 595379.6250\n",
            "Epoch 200/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1288.3956 - val_loss: 346052.9062\n",
            "Epoch 201/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1264.4852 - val_loss: 627081.8125\n",
            "Epoch 202/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1254.4641 - val_loss: 377798.0000\n",
            "Epoch 203/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1294.5886 - val_loss: 308777.8125\n",
            "Epoch 204/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1288.5227 - val_loss: 415337.8438\n",
            "Epoch 205/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1286.1049 - val_loss: 332218.2500\n",
            "Epoch 206/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1303.3301 - val_loss: 389663.9688\n",
            "Epoch 207/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1267.9626 - val_loss: 745840.8125\n",
            "Epoch 208/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1314.7330 - val_loss: 559383.3125\n",
            "Epoch 209/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1291.1167 - val_loss: 312227.0938\n",
            "Epoch 210/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1276.7625 - val_loss: 511237.8125\n",
            "Epoch 211/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1270.8792 - val_loss: 322408.2188\n",
            "Epoch 212/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1289.9384 - val_loss: 458492.4375\n",
            "Epoch 213/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1291.7266 - val_loss: 367261.7500\n",
            "Epoch 214/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1249.3530 - val_loss: 514572.8438\n",
            "Epoch 215/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1272.2766 - val_loss: 352801.8750\n",
            "Epoch 216/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1339.3491 - val_loss: 342986.4688\n",
            "Epoch 217/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1278.4178 - val_loss: 562243.5625\n",
            "Epoch 218/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1312.4252 - val_loss: 525132.6875\n",
            "Epoch 219/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1312.9712 - val_loss: 322630.7188\n",
            "Epoch 220/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1248.6293 - val_loss: 384021.4688\n",
            "Epoch 221/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1228.0447 - val_loss: 371706.5312\n",
            "Epoch 222/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1279.9005 - val_loss: 486016.3750\n",
            "Epoch 223/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1279.2340 - val_loss: 348187.6875\n",
            "Epoch 224/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1302.3191 - val_loss: 298854.9062\n",
            "Epoch 225/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1249.4032 - val_loss: 472247.6875\n",
            "Epoch 226/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1278.7218 - val_loss: 391566.1875\n",
            "Epoch 227/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1293.3932 - val_loss: 367269.8125\n",
            "Epoch 228/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1279.3063 - val_loss: 395445.4062\n",
            "Epoch 229/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1294.4098 - val_loss: 519655.0000\n",
            "Epoch 230/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1322.6229 - val_loss: 522860.2188\n",
            "Epoch 231/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1240.0310 - val_loss: 343975.7188\n",
            "Epoch 232/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1257.7390 - val_loss: 401851.1875\n",
            "Epoch 233/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1246.2078 - val_loss: 590662.4375\n",
            "Epoch 234/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1256.1161 - val_loss: 377376.9375\n",
            "Epoch 235/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1295.8619 - val_loss: 655615.3750\n",
            "Epoch 236/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1258.7897 - val_loss: 548343.4375\n",
            "Epoch 237/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1280.8055 - val_loss: 487649.0625\n",
            "Epoch 238/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1342.2239 - val_loss: 601492.9375\n",
            "Epoch 239/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1308.6720 - val_loss: 414011.1875\n",
            "Epoch 240/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1254.0498 - val_loss: 503246.0312\n",
            "Epoch 241/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1276.6957 - val_loss: 418371.1875\n",
            "Epoch 242/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1282.3561 - val_loss: 339195.7500\n",
            "Epoch 243/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1243.5927 - val_loss: 345329.3750\n",
            "Epoch 244/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1288.4178 - val_loss: 373675.8438\n",
            "Epoch 245/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1232.7284 - val_loss: 291848.8750\n",
            "Epoch 246/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1239.5657 - val_loss: 362363.9688\n",
            "Epoch 247/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1267.3080 - val_loss: 551229.6250\n",
            "Epoch 248/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1321.3590 - val_loss: 383889.9688\n",
            "Epoch 249/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1283.2052 - val_loss: 423613.1875\n",
            "Epoch 250/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1309.1896 - val_loss: 396448.6875\n",
            "Epoch 251/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1247.8647 - val_loss: 473296.0625\n",
            "Epoch 252/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1299.1180 - val_loss: 459801.0625\n",
            "Epoch 253/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1283.4602 - val_loss: 491207.2500\n",
            "Epoch 254/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1272.0912 - val_loss: 482243.9062\n",
            "Epoch 255/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1260.4856 - val_loss: 416498.4688\n",
            "Epoch 256/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1262.0272 - val_loss: 360491.6562\n",
            "Epoch 257/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1257.2898 - val_loss: 476126.0938\n",
            "Epoch 258/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1304.0659 - val_loss: 603671.8125\n",
            "Epoch 259/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1226.7814 - val_loss: 371113.5000\n",
            "Epoch 260/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1277.1702 - val_loss: 376530.6562\n",
            "Epoch 261/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1256.9130 - val_loss: 411520.6250\n",
            "Epoch 262/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1254.6177 - val_loss: 413268.4375\n",
            "Epoch 263/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1306.1130 - val_loss: 505625.7188\n",
            "Epoch 264/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1264.5769 - val_loss: 620002.3750\n",
            "Epoch 265/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1225.8704 - val_loss: 395904.9062\n",
            "Epoch 266/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1330.5316 - val_loss: 373144.8750\n",
            "Epoch 267/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1253.2743 - val_loss: 478156.0000\n",
            "Epoch 268/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1200.0909 - val_loss: 432034.2188\n",
            "Epoch 269/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1247.8423 - val_loss: 401746.5625\n",
            "Epoch 270/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1269.3253 - val_loss: 342135.0000\n",
            "Epoch 271/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1198.9790 - val_loss: 643005.6250\n",
            "Epoch 272/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1290.0697 - val_loss: 690252.1250\n",
            "Epoch 273/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1358.1254 - val_loss: 617338.6250\n",
            "Epoch 274/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1303.9550 - val_loss: 432937.5312\n",
            "Epoch 275/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1288.5315 - val_loss: 562020.4375\n",
            "Epoch 276/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1243.8123 - val_loss: 463246.7812\n",
            "Epoch 277/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1261.2262 - val_loss: 680617.8750\n",
            "Epoch 278/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1252.9849 - val_loss: 604747.6250\n",
            "Epoch 279/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1224.3402 - val_loss: 381167.9688\n",
            "Epoch 280/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1307.9799 - val_loss: 581825.8750\n",
            "Epoch 281/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1292.0153 - val_loss: 588922.0000\n",
            "Epoch 282/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1226.3553 - val_loss: 405155.8438\n",
            "Epoch 283/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1220.9772 - val_loss: 953659.9375\n",
            "Epoch 284/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1253.6841 - val_loss: 787386.3125\n",
            "Epoch 285/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1232.1608 - val_loss: 456628.7500\n",
            "Epoch 286/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1268.7926 - val_loss: 600750.0000\n",
            "Epoch 287/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1214.0060 - val_loss: 612328.5625\n",
            "Epoch 288/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1230.8988 - val_loss: 677216.9375\n",
            "Epoch 289/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1215.8549 - val_loss: 709931.4375\n",
            "Epoch 290/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1212.0627 - val_loss: 621978.1875\n",
            "Epoch 291/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1217.7405 - val_loss: 392149.8438\n",
            "Epoch 292/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1244.9163 - val_loss: 557397.9375\n",
            "Epoch 293/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1181.9847 - val_loss: 625017.4375\n",
            "Epoch 294/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1193.8508 - val_loss: 350987.7188\n",
            "Epoch 295/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1184.8068 - val_loss: 373796.9062\n",
            "Epoch 296/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1180.5785 - val_loss: 309910.8438\n",
            "Epoch 297/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1216.2780 - val_loss: 340860.0625\n",
            "Epoch 298/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1198.6241 - val_loss: 438640.4688\n",
            "Epoch 299/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1201.8160 - val_loss: 489472.8125\n",
            "Epoch 300/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1204.1458 - val_loss: 440179.5312\n",
            "Epoch 301/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1172.1581 - val_loss: 401851.3750\n",
            "Epoch 302/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1143.4369 - val_loss: 341885.3125\n",
            "Epoch 303/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1189.2396 - val_loss: 520398.2500\n",
            "Epoch 304/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1228.1859 - val_loss: 504309.0000\n",
            "Epoch 305/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1233.4751 - val_loss: 352295.2812\n",
            "Epoch 306/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1181.0472 - val_loss: 537360.2500\n",
            "Epoch 307/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1213.8701 - val_loss: 616657.7500\n",
            "Epoch 308/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1184.2124 - val_loss: 509634.6250\n",
            "Epoch 309/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1184.3257 - val_loss: 481864.6875\n",
            "Epoch 310/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1185.6689 - val_loss: 626641.8125\n",
            "Epoch 311/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1224.7617 - val_loss: 438161.7812\n",
            "Epoch 312/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1137.8992 - val_loss: 392442.9062\n",
            "Epoch 313/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1190.8582 - val_loss: 549926.0000\n",
            "Epoch 314/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1195.6719 - val_loss: 548717.7500\n",
            "Epoch 315/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1153.5695 - val_loss: 364127.9688\n",
            "Epoch 316/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1241.4523 - val_loss: 570908.4375\n",
            "Epoch 317/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1184.9357 - val_loss: 523662.2812\n",
            "Epoch 318/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1159.5658 - val_loss: 583638.8125\n",
            "Epoch 319/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1248.0636 - val_loss: 387134.9062\n",
            "Epoch 320/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1211.0211 - val_loss: 424519.6875\n",
            "Epoch 321/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1157.0580 - val_loss: 691821.5000\n",
            "Epoch 322/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1162.8549 - val_loss: 379156.6250\n",
            "Epoch 323/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1197.4298 - val_loss: 589540.7500\n",
            "Epoch 324/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1159.6466 - val_loss: 414159.7188\n",
            "Epoch 325/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1154.5529 - val_loss: 571029.1250\n",
            "Epoch 326/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1180.8036 - val_loss: 440807.9688\n",
            "Epoch 327/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1202.0132 - val_loss: 535529.5000\n",
            "Epoch 328/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1169.9436 - val_loss: 406840.6250\n",
            "Epoch 329/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1178.0555 - val_loss: 460137.2188\n",
            "Epoch 330/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1150.8981 - val_loss: 512927.3125\n",
            "Epoch 331/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1221.1132 - val_loss: 666628.3750\n",
            "Epoch 332/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1209.7056 - val_loss: 604551.8125\n",
            "Epoch 333/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1166.4226 - val_loss: 545554.0000\n",
            "Epoch 334/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1166.1937 - val_loss: 586026.4375\n",
            "Epoch 335/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1146.4137 - val_loss: 422161.0000\n",
            "Epoch 336/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1161.4536 - val_loss: 631258.7500\n",
            "Epoch 337/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1209.4523 - val_loss: 529059.3750\n",
            "Epoch 338/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1240.2362 - val_loss: 471020.6250\n",
            "Epoch 339/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1189.3812 - val_loss: 454196.8125\n",
            "Epoch 340/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1231.8533 - val_loss: 397614.4688\n",
            "Epoch 341/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1178.1661 - val_loss: 433226.2188\n",
            "Epoch 342/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1190.2909 - val_loss: 413628.4375\n",
            "Epoch 343/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1213.2478 - val_loss: 500033.5938\n",
            "Epoch 344/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1150.4180 - val_loss: 508960.4688\n",
            "Epoch 345/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1174.4056 - val_loss: 576713.5625\n",
            "Epoch 346/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1225.5143 - val_loss: 480804.3125\n",
            "Epoch 347/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1160.4589 - val_loss: 464159.4688\n",
            "Epoch 348/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1165.9323 - val_loss: 550350.5000\n",
            "Epoch 349/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1170.1791 - val_loss: 508485.5000\n",
            "Epoch 350/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1211.7349 - val_loss: 624085.8125\n",
            "Epoch 351/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1205.9192 - val_loss: 640980.8750\n",
            "Epoch 352/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1161.5272 - val_loss: 497618.1250\n",
            "Epoch 353/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1174.9769 - val_loss: 549002.6250\n",
            "Epoch 354/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1218.9294 - val_loss: 690599.8125\n",
            "Epoch 355/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1166.7035 - val_loss: 438196.7812\n",
            "Epoch 356/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1242.1346 - val_loss: 437277.4062\n",
            "Epoch 357/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1172.7028 - val_loss: 389408.2812\n",
            "Epoch 358/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1226.6388 - val_loss: 544611.3750\n",
            "Epoch 359/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1189.1116 - val_loss: 407096.7812\n",
            "Epoch 360/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1133.5889 - val_loss: 455417.5000\n",
            "Epoch 361/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1167.6104 - val_loss: 422301.6562\n",
            "Epoch 362/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1149.3228 - val_loss: 572938.2500\n",
            "Epoch 363/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1113.9661 - val_loss: 501530.7188\n",
            "Epoch 364/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1121.3546 - val_loss: 458652.5938\n",
            "Epoch 365/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1124.2065 - val_loss: 410191.7188\n",
            "Epoch 366/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1114.3993 - val_loss: 422532.5938\n",
            "Epoch 367/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1098.0583 - val_loss: 441701.6562\n",
            "Epoch 368/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1132.0508 - val_loss: 477768.2812\n",
            "Epoch 369/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1082.8881 - val_loss: 387551.3438\n",
            "Epoch 370/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1100.2057 - val_loss: 526646.7500\n",
            "Epoch 371/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1107.9091 - val_loss: 463188.6875\n",
            "Epoch 372/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1087.9128 - val_loss: 390769.5312\n",
            "Epoch 373/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1099.6271 - val_loss: 442490.5938\n",
            "Epoch 374/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1081.1396 - val_loss: 469859.1875\n",
            "Epoch 375/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1099.6478 - val_loss: 531985.9375\n",
            "Epoch 376/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1119.6216 - val_loss: 602644.9375\n",
            "Epoch 377/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1112.3086 - val_loss: 378794.8438\n",
            "Epoch 378/1000\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 1112.1108 - val_loss: 421027.7188\n",
            "Epoch 379/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1133.3893 - val_loss: 423629.3750\n",
            "Epoch 380/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1081.7200 - val_loss: 627048.8750\n",
            "Epoch 381/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1090.3009 - val_loss: 526803.9375\n",
            "Epoch 382/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1120.1383 - val_loss: 511188.6250\n",
            "Epoch 383/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1107.5592 - val_loss: 531281.8125\n",
            "Epoch 384/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1096.8879 - val_loss: 476755.7812\n",
            "Epoch 385/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1102.5092 - val_loss: 553620.6250\n",
            "Epoch 386/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1111.6177 - val_loss: 522310.1875\n",
            "Epoch 387/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1150.2452 - val_loss: 483610.4688\n",
            "Epoch 388/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1093.9934 - val_loss: 552079.9375\n",
            "Epoch 389/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1098.4380 - val_loss: 404738.4688\n",
            "Epoch 390/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1090.1899 - val_loss: 551050.7500\n",
            "Epoch 391/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1108.4315 - val_loss: 508868.6250\n",
            "Epoch 392/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1140.6094 - val_loss: 510112.0625\n",
            "Epoch 393/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1095.8113 - val_loss: 506363.6250\n",
            "Epoch 394/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1094.7056 - val_loss: 520607.7812\n",
            "Epoch 395/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1107.1954 - val_loss: 498481.2188\n",
            "Epoch 396/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1078.7957 - val_loss: 519242.8125\n",
            "Epoch 397/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1100.0204 - val_loss: 585343.1250\n",
            "Epoch 398/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1120.0699 - val_loss: 565599.8750\n",
            "Epoch 399/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1089.1971 - val_loss: 638073.5625\n",
            "Epoch 400/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1074.6559 - val_loss: 533051.1250\n",
            "Epoch 401/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1118.2906 - val_loss: 470141.3750\n",
            "Epoch 402/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1105.6611 - val_loss: 423156.2188\n",
            "Epoch 403/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1070.9286 - val_loss: 620364.5625\n",
            "Epoch 404/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1087.9227 - val_loss: 639993.3750\n",
            "Epoch 405/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1100.7963 - val_loss: 373720.5938\n",
            "Epoch 406/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1120.5927 - val_loss: 490112.9062\n",
            "Epoch 407/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1076.2776 - val_loss: 456381.7188\n",
            "Epoch 408/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1123.2340 - val_loss: 545230.4375\n",
            "Epoch 409/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1071.2141 - val_loss: 536077.1875\n",
            "Epoch 410/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1074.3768 - val_loss: 470115.3125\n",
            "Epoch 411/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1085.2175 - val_loss: 444753.3125\n",
            "Epoch 412/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1121.2504 - val_loss: 482106.4062\n",
            "Epoch 413/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1084.2168 - val_loss: 455511.6875\n",
            "Epoch 414/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1098.3729 - val_loss: 529648.3125\n",
            "Epoch 415/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1082.2325 - val_loss: 480147.4688\n",
            "Epoch 416/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1110.1172 - val_loss: 426769.5938\n",
            "Epoch 417/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1116.6412 - val_loss: 567098.3750\n",
            "Epoch 418/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1106.8875 - val_loss: 569407.3125\n",
            "Epoch 419/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1110.6956 - val_loss: 410713.8438\n",
            "Epoch 420/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1082.8740 - val_loss: 542614.0000\n",
            "Epoch 421/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1103.6622 - val_loss: 403514.8438\n",
            "Epoch 422/1000\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 1052.6431 - val_loss: 425863.3750\n",
            "Epoch 423/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1121.7509 - val_loss: 548430.9375\n",
            "Epoch 424/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1078.7841 - val_loss: 527410.1875\n",
            "Epoch 425/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1085.6472 - val_loss: 564715.4375\n",
            "Epoch 426/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1068.7906 - val_loss: 482937.6562\n",
            "Epoch 427/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1115.8014 - val_loss: 524198.1250\n",
            "Epoch 428/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1094.6123 - val_loss: 519326.8750\n",
            "Epoch 429/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1098.3557 - val_loss: 587251.0625\n",
            "Epoch 430/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1112.5571 - val_loss: 419081.0625\n",
            "Epoch 431/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1055.8644 - val_loss: 608893.6250\n",
            "Epoch 432/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1127.1307 - val_loss: 442431.4688\n",
            "Epoch 433/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1067.6707 - val_loss: 420537.9062\n",
            "Epoch 434/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1074.9388 - val_loss: 448507.0000\n",
            "Epoch 435/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1105.6107 - val_loss: 472291.7812\n",
            "Epoch 436/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1081.8046 - val_loss: 538707.3125\n",
            "Epoch 437/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1078.5601 - val_loss: 453710.9375\n",
            "Epoch 438/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1110.8673 - val_loss: 699676.9375\n",
            "Epoch 439/1000\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 1061.1219 - val_loss: 529169.0000\n",
            "Epoch 440/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1088.1609 - val_loss: 617994.0000\n",
            "Epoch 441/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1107.7844 - val_loss: 705109.6875\n",
            "Epoch 442/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1128.5393 - val_loss: 496162.8125\n",
            "Epoch 443/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1122.3610 - val_loss: 704918.4375\n",
            "Epoch 444/1000\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 1067.5331 - val_loss: 446404.9688\n",
            "Epoch 445/1000\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1086.0536 - val_loss: 525167.3125\n",
            "Epoch 446/1000\n",
            "600/600 [==============================] - 7s 11ms/step - loss: 1123.9119 - val_loss: 430128.2812\n",
            "Epoch 447/1000\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 1094.2133 - val_loss: 455292.0000\n",
            "Epoch 448/1000\n",
            "429/600 [====================>.........] - ETA: 1s - loss: 1048.8805"
          ]
        }
      ],
      "source": [
        "list = ['s30', 's40', 's50', 's70', 's100', 'c30', 'c40', 'c50', 'c70', 'c100'] #모든 파일에 대해서 for 문 돌려서 실행\n",
        "count = 1\n",
        "for i in list :\n",
        "\n",
        "    # 데이터 불러오기\n",
        "    lane_data_c = pd.read_csv('lane_data_c.csv', encoding='utf-8')\n",
        "    lane_data_s = pd.read_csv('lane_data_s.csv', encoding='utf-8')\n",
        "    data = pd.read_csv('data_'+i+'.csv', encoding='utf-8') #for문을 이용하여 데이터 바꾸기\n",
        "    # 데이터 결합\n",
        "    data_combined = pd.concat([lane_data_c, lane_data_s, data], axis=1) #데이터합치기\n",
        "    data_combined = data_combined.loc[:, ~data_combined.columns.duplicated()]#거리데이터 중복되어있으니까 빼기\n",
        "    data_combined = data_combined.drop_duplicates(subset='Distance', keep='first')  # 첫 번째 중복 행만 남기기\n",
        "\n",
        "\n",
        "\n",
        "    # 입력 변수와 탈선계수 분리\n",
        "    X_time_series = data_combined[['Distance']]  #Distance를 시계열 데이터로 쓰기 위해서 떼어내기\n",
        "    X_features = data_combined.drop(['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2', 'Distance'], axis=1) #라벨링데이터 떼어내기\n",
        "    y = data_combined[['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']] #라벨링하기\n",
        "\n",
        "\n",
        "    # 데이터 정규화\n",
        "    scaler = MinMaxScaler() # 스케일링하기\n",
        "    X_features_scaled = scaler.fit_transform(X_features) #스케일링\n",
        "\n",
        "\n",
        "    # 학습 데이터와 테스트 데이터 분할\n",
        "    X_features_train, X_features_test, X_time_series_train, X_time_series_test, y_train, y_test = train_test_split(X_features_scaled, X_time_series, y, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "    # # 입력 정의\n",
        "    # input_time_series = Input(shape=(X_time_series.shape[1], 1), name='input_time_series')\n",
        "    # input_features = Input(shape=(X_features_train.shape[1],), name='input_features')\n",
        "\n",
        "    # # 시계열 데이터 처리를 위한 LSTM 층\n",
        "    # lstm_units = 64\n",
        "    # lstm_output = LSTM(units=lstm_units, activation='tanh', return_sequences=True)(input_time_series)\n",
        "    # lstm_output = LSTM(units=lstm_units, activation='tanh')(lstm_output)\n",
        "\n",
        "    # # 특성 데이터 처리를 위한 밀집층\n",
        "    # features_output = Dense(units=32, activation='tanh')(input_features)\n",
        "\n",
        "    # # LSTM 층과 밀집층을 합치기\n",
        "    # concatenated = concatenate([lstm_output, features_output])\n",
        "\n",
        "    # # 예측을 위한 밀집층 추가\n",
        "    # output_layer = Dense(4)(concatenated)  # 4개의 탈선계수를 예측하므로 출력 뉴런 수는 4\n",
        "\n",
        "    # # 모델 구성\n",
        "    # model = Model(inputs=[input_time_series, input_features], outputs=output_layer)\n",
        "\n",
        "    # # 모델 컴파일\n",
        "    # model.compile(optimizer='adam', loss=weighted_mape_loss)\n",
        "\n",
        "\n",
        "    # 하이퍼파라미터 튜닝을 위한 함수 정의\n",
        "    def build_model(hp):\n",
        "        lstm_units = hp.Int('lstm_units', min_value=16, max_value=128, step=8)\n",
        "        dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.005)\n",
        "        num_layers = hp.Int('num_layers', min_value=1, max_value=4, step=1)\n",
        "        optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
        "        learning_rate = hp.Choice('learning_rate', values=[1e-1,1e-2,1e-3,1e-4])\n",
        "\n",
        "        input_time_series = Input(shape=(X_time_series_train.shape[1], 1), name='input_time_series')\n",
        "        input_features = Input(shape=(X_features_train.shape[1],), name='input_features')  # 추가된 입력\n",
        "\n",
        "        lstm_output = LSTM(units=lstm_units, activation='tanh', return_sequences=True)(input_time_series)\n",
        "        for _ in range(num_layers):\n",
        "            lstm_output = LSTM(units=lstm_units, activation='tanh', return_sequences=True)(lstm_output)\n",
        "            lstm_output = Dropout(dropout_rate)(lstm_output)\n",
        "        lstm_output = LSTM(units=lstm_units, activation='tanh')(lstm_output)\n",
        "\n",
        "        features_output = Dense(units=32, activation='tanh')(input_features)\n",
        "\n",
        "        concatenated = concatenate([lstm_output, features_output])\n",
        "\n",
        "        output_layer = Dense(4)(concatenated)\n",
        "\n",
        "        tuned_model = Model(inputs=[input_time_series, input_features], outputs=output_layer)\n",
        "        tuned_model.compile(optimizer=optimizer, loss=weighted_mape_loss)\n",
        "#####################################################################################\n",
        "        return tuned_model\n",
        "\n",
        "    epochs = 1000\n",
        "    batch_size = 16\n",
        "\n",
        "    tuner = RandomSearch(build_model, objective='val_loss', max_trials=20, executions_per_trial=1, directory='tuner_results', project_name='model_tuning')\n",
        "\n",
        "    # 튜닝할 파라미터 정의\n",
        "    tuner.search_space_summary()\n",
        "\n",
        "    # 튜닝 실행\n",
        "    tuner.search([X_time_series_train, X_features_train], y_train, epochs=epochs, batch_size=batch_size, validation_data=([X_time_series_test, X_features_test], y_test))\n",
        "\n",
        "\n",
        "    # 최적의 모델 선택\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    best_model.fit(\n",
        "        [X_time_series_train, X_features_train], y_train,\n",
        "        epochs=1000,\n",
        "        batch_size=16,  # Use the best batch size\n",
        "        validation_data=([X_time_series_test, X_features_test], y_test),\n",
        "        callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
        "    )\n",
        "\n",
        "    # 다음 1999개의 샘플 예측\n",
        "    next_samples = 1999\n",
        "    X_time_series_predict = X_time_series[-next_samples:]\n",
        "    X_features_predict = X_features[-next_samples:]\n",
        "\n",
        "    # 모델 예측\n",
        "    predictions = best_model.predict([X_time_series_predict, X_features_predict])\n",
        "\n",
        "    answer_sample = pd.read_csv('answer_sample.csv', header=None)\n",
        "    answer_sample.iloc[1:, count:count+4] = predictions  # 예측 결과 저장\n",
        "    answer_sample.to_csv('answer_sample.csv', index=False, header=False)  # 결과를 파일에 저장\n",
        "    count = count + 4\n",
        "    #튜너 삭제\n",
        "    shutil.rmtree('tuner_results')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "uT31GG13v0Dz",
        "outputId": "02b14aaa-f2da-4786-d8fb-79308b497971"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-22fffedc-d76e-4758-b252-704ad082bcee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Distance</td>\n",
              "      <td>YL_M1_B1_W1_s30</td>\n",
              "      <td>YR_M1_B1_W1_s30</td>\n",
              "      <td>YL_M1_B1_W2_s30</td>\n",
              "      <td>YR_M1_B1_W2_s30</td>\n",
              "      <td>YL_M1_B1_W1_s40</td>\n",
              "      <td>YR_M1_B1_W1_s40</td>\n",
              "      <td>YL_M1_B1_W2_s40</td>\n",
              "      <td>YR_M1_B1_W2_s40</td>\n",
              "      <td>YL_M1_B1_W1_s50</td>\n",
              "      <td>...</td>\n",
              "      <td>YL_M1_B1_W2_c50</td>\n",
              "      <td>YR_M1_B1_W2_c50</td>\n",
              "      <td>YL_M1_B1_W1_c70</td>\n",
              "      <td>YR_M1_B1_W1_c70</td>\n",
              "      <td>YL_M1_B1_W2_c70</td>\n",
              "      <td>YR_M1_B1_W2_c70</td>\n",
              "      <td>YL_M1_B1_W1_c100</td>\n",
              "      <td>YR_M1_B1_W1_c100</td>\n",
              "      <td>YL_M1_B1_W2_c100</td>\n",
              "      <td>YR_M1_B1_W2_c100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2500.25</td>\n",
              "      <td>3.0383856296539307</td>\n",
              "      <td>3.3745944499969482</td>\n",
              "      <td>3.150264024734497</td>\n",
              "      <td>0.5677508115768433</td>\n",
              "      <td>1.590240240097046</td>\n",
              "      <td>-0.8460798859596252</td>\n",
              "      <td>0.35806435346603394</td>\n",
              "      <td>-0.8905683755874634</td>\n",
              "      <td>1.219680666923523</td>\n",
              "      <td>...</td>\n",
              "      <td>0.06297896802425385</td>\n",
              "      <td>-0.2666114270687103</td>\n",
              "      <td>-0.3706197142601013</td>\n",
              "      <td>2.079420566558838</td>\n",
              "      <td>-0.5998522639274597</td>\n",
              "      <td>-0.005573870614171028</td>\n",
              "      <td>-0.2598481774330139</td>\n",
              "      <td>-0.10763148963451385</td>\n",
              "      <td>0.46673956513404846</td>\n",
              "      <td>0.13719095289707184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2500.5</td>\n",
              "      <td>4.388960361480713</td>\n",
              "      <td>4.988764762878418</td>\n",
              "      <td>2.9292871952056885</td>\n",
              "      <td>1.6415818929672241</td>\n",
              "      <td>1.1254503726959229</td>\n",
              "      <td>-0.5474135279655457</td>\n",
              "      <td>-0.14807218313217163</td>\n",
              "      <td>0.043817199766635895</td>\n",
              "      <td>1.1105281114578247</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8448710441589355</td>\n",
              "      <td>0.9835374355316162</td>\n",
              "      <td>0.04862434044480324</td>\n",
              "      <td>3.5793468952178955</td>\n",
              "      <td>-1.1364562511444092</td>\n",
              "      <td>1.1456530094146729</td>\n",
              "      <td>-0.9671570658683777</td>\n",
              "      <td>0.6867867112159729</td>\n",
              "      <td>-0.9352637529373169</td>\n",
              "      <td>0.5585556030273438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2500.75</td>\n",
              "      <td>4.857058048248291</td>\n",
              "      <td>7.091855049133301</td>\n",
              "      <td>1.1406546831130981</td>\n",
              "      <td>1.1984490156173706</td>\n",
              "      <td>1.1719924211502075</td>\n",
              "      <td>-0.15573377907276154</td>\n",
              "      <td>-0.24037331342697144</td>\n",
              "      <td>-0.008829018101096153</td>\n",
              "      <td>-0.20981845259666443</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.7839117646217346</td>\n",
              "      <td>1.1493388414382935</td>\n",
              "      <td>-1.4068650007247925</td>\n",
              "      <td>4.369986057281494</td>\n",
              "      <td>-1.971381425857544</td>\n",
              "      <td>1.2050524950027466</td>\n",
              "      <td>-1.8323396444320679</td>\n",
              "      <td>2.6019110679626465</td>\n",
              "      <td>-1.3256511688232422</td>\n",
              "      <td>0.9894505739212036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2501.0</td>\n",
              "      <td>4.885616779327393</td>\n",
              "      <td>7.02304220199585</td>\n",
              "      <td>0.2526165544986725</td>\n",
              "      <td>-0.08797340095043182</td>\n",
              "      <td>0.9910439848899841</td>\n",
              "      <td>0.16068248450756073</td>\n",
              "      <td>0.23632007837295532</td>\n",
              "      <td>-0.9893527030944824</td>\n",
              "      <td>-0.24068251252174377</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.11427594721317291</td>\n",
              "      <td>0.34817755222320557</td>\n",
              "      <td>-1.26677668094635</td>\n",
              "      <td>3.9846315383911133</td>\n",
              "      <td>-0.33597585558891296</td>\n",
              "      <td>-0.21141111850738525</td>\n",
              "      <td>-2.2528817653656006</td>\n",
              "      <td>3.0522849559783936</td>\n",
              "      <td>-0.5561127662658691</td>\n",
              "      <td>0.32541918754577637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 41 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22fffedc-d76e-4758-b252-704ad082bcee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-482efff6-ddfd-4e49-ba42-3adc2e2fdff3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-482efff6-ddfd-4e49-ba42-3adc2e2fdff3')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-482efff6-ddfd-4e49-ba42-3adc2e2fdff3 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22fffedc-d76e-4758-b252-704ad082bcee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22fffedc-d76e-4758-b252-704ad082bcee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         0                   1                   2                   3   \\\n",
              "0  Distance     YL_M1_B1_W1_s30     YR_M1_B1_W1_s30     YL_M1_B1_W2_s30   \n",
              "1   2500.25  3.0383856296539307  3.3745944499969482   3.150264024734497   \n",
              "2    2500.5   4.388960361480713   4.988764762878418  2.9292871952056885   \n",
              "3   2500.75   4.857058048248291   7.091855049133301  1.1406546831130981   \n",
              "4    2501.0   4.885616779327393    7.02304220199585  0.2526165544986725   \n",
              "\n",
              "                     4                   5                     6   \\\n",
              "0       YR_M1_B1_W2_s30     YL_M1_B1_W1_s40       YR_M1_B1_W1_s40   \n",
              "1    0.5677508115768433   1.590240240097046   -0.8460798859596252   \n",
              "2    1.6415818929672241  1.1254503726959229   -0.5474135279655457   \n",
              "3    1.1984490156173706  1.1719924211502075  -0.15573377907276154   \n",
              "4  -0.08797340095043182  0.9910439848899841   0.16068248450756073   \n",
              "\n",
              "                     7                      8                     9   ...  \\\n",
              "0       YL_M1_B1_W2_s40        YR_M1_B1_W2_s40       YL_M1_B1_W1_s50  ...   \n",
              "1   0.35806435346603394    -0.8905683755874634     1.219680666923523  ...   \n",
              "2  -0.14807218313217163   0.043817199766635895    1.1105281114578247  ...   \n",
              "3  -0.24037331342697144  -0.008829018101096153  -0.20981845259666443  ...   \n",
              "4   0.23632007837295532    -0.9893527030944824  -0.24068251252174377  ...   \n",
              "\n",
              "                     31                   32                   33  \\\n",
              "0       YL_M1_B1_W2_c50      YR_M1_B1_W2_c50      YL_M1_B1_W1_c70   \n",
              "1   0.06297896802425385  -0.2666114270687103  -0.3706197142601013   \n",
              "2   -0.8448710441589355   0.9835374355316162  0.04862434044480324   \n",
              "3   -0.7839117646217346   1.1493388414382935  -1.4068650007247925   \n",
              "4  -0.11427594721317291  0.34817755222320557    -1.26677668094635   \n",
              "\n",
              "                   34                    35                     36  \\\n",
              "0     YR_M1_B1_W1_c70       YL_M1_B1_W2_c70        YR_M1_B1_W2_c70   \n",
              "1   2.079420566558838   -0.5998522639274597  -0.005573870614171028   \n",
              "2  3.5793468952178955   -1.1364562511444092     1.1456530094146729   \n",
              "3   4.369986057281494    -1.971381425857544     1.2050524950027466   \n",
              "4  3.9846315383911133  -0.33597585558891296   -0.21141111850738525   \n",
              "\n",
              "                    37                    38                   39  \\\n",
              "0     YL_M1_B1_W1_c100      YR_M1_B1_W1_c100     YL_M1_B1_W2_c100   \n",
              "1  -0.2598481774330139  -0.10763148963451385  0.46673956513404846   \n",
              "2  -0.9671570658683777    0.6867867112159729  -0.9352637529373169   \n",
              "3  -1.8323396444320679    2.6019110679626465  -1.3256511688232422   \n",
              "4  -2.2528817653656006    3.0522849559783936  -0.5561127662658691   \n",
              "\n",
              "                    40  \n",
              "0     YR_M1_B1_W2_c100  \n",
              "1  0.13719095289707184  \n",
              "2   0.5585556030273438  \n",
              "3   0.9894505739212036  \n",
              "4  0.32541918754577637  \n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "answer_sample = pd.read_csv('answer_sample.csv', header=None)\n",
        "answer_sample.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN51XWqbo3r/B2hWOyX1AYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}